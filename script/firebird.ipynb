{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script de Firebird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script se ejecuta diariamente para procesar todas las campañas de márketing activas.\n",
    "\n",
    "En primer lugar descarga de Firestore la lista de campañas activas. Para cada una de ellas:\n",
    "\n",
    "1. Obtiene una muestra de tweets relevantes para la campaña.\n",
    "2. Analiza los tweets mediante un modelo PLN.\n",
    "3. Procesa los resultados para obtener conclusiones sobre el total de los datos, y agrupadas por provincias.\n",
    "4. Almacena los resultados en Firestore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrés Fernández\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Importar librerías y credenciales\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    import tweepy\n",
    "    from firebase_admin import credentials, firestore, initialize_app\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "except ImportError:\n",
    "    raise ImportError(\"La aplicación requiere las librerías tweepy, firebase_admin y transformers.\")    \n",
    "\n",
    "try:\n",
    "    import keys\n",
    "except ImportError:\n",
    "    raise ImportError(\"No se encuentra el archivo 'keys.py' con las credenciales de la API de Twitter.\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import datetime, os\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Descargar modelo (sólo si no se ha descargado previamente)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_path = './models/transformers/' \n",
    "\n",
    "if not os.path.exists('./models/transformers'):\n",
    "    \n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "    classifier.save_pretrained(model_path)\n",
    "\n",
    "    logging.info(\"Modelo descargado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inicializando servicios...\n",
      "INFO:root:Firestore inicializado.\n",
      "INFO:root:Tweepy inicializado.\n",
      "INFO:root:Modelo NLP inicializado.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Inicializar servicios\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "logging.info(\"Inicializando servicios...\")\n",
    "\n",
    "# Firestore\n",
    "cred = credentials.Certificate('service-acc-key.json')\n",
    "initialize_app(cred)\n",
    "db = firestore.client()\n",
    "logging.info(\"Firestore inicializado.\")\n",
    "\n",
    "# Tweepy\n",
    "client = tweepy.Client(bearer_token=keys.bearer, access_token=keys.access_token, access_token_secret=keys.access_token_secret)\n",
    "logging.info(\"Tweepy inicializado.\")\n",
    "\n",
    "# Clasificador\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "logging.info(\"Modelo NLP inicializado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Inicializar fechas\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "currentTimezone = datetime.datetime.now().astimezone().tzinfo\n",
    "currentTime = datetime.datetime.now(tz=currentTimezone)\n",
    "currentDay = datetime.datetime(currentTime.year, currentTime.month, currentTime.day, tzinfo=currentTimezone)\n",
    "targetDay = currentDay - datetime.timedelta(days=1)\n",
    "\n",
    "# Fechas en formato ISO (YYYY-MM-DD), que utilizo como identificadores en Firestore\n",
    "dateCode = targetDay.date().isoformat()\n",
    "\n",
    "listaProvincias = [\"A Coruña\", \"Álava\", \"Albacete\", \"Alicante\", \"Almería\", \"Asturias\", \"Ávila\", \"Badajoz\", \"Baleares\", \"Barcelona\", \"Burgos\", \"Cáceres\", \"Cádiz\", \"Cantabria\", \"Castellón\", \"Ciudad Real\", \"Córdoba\", \"Cuenca\", \"Girona\", \"Granada\", \"Guadalajara\", \"Gipuzkoa\", \"Huelva\", \"Huesca\", \"Jaén\", \"La Rioja\", \"Las Palmas\", \"León\", \"Lérida\", \"Lugo\", \"Madrid\", \"Málaga\", \"Murcia\", \"Navarra\", \"Ourense\", \"Palencia\", \"Pontevedra\", \"Salamanca\", \"Segovia\", \"Sevilla\", \"Soria\", \"Tarragona\", \"Santa Cruz de Tenerife\", \"Teruel\", \"Toledo\", \"Valencia\", \"Valladolid\", \"Vizcaya\", \"Zamora\", \"Zaragoza\"]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Recopilar tweets\n",
    "# -----------------------------------------------------------------------------\n",
    "import math\n",
    "\n",
    "def collectTweets(query, numTweets):    \n",
    "\n",
    "    tweetIDs = []\n",
    "    tweetTexts = []\n",
    "    tweetLocations = []\n",
    "\n",
    "    listaUsuarios = []\n",
    "\n",
    "    if numTweets > 1000:\n",
    "        raise Exception(\"Para evitar gastos, la aplicación no admite analizar más de 1000 tweets por campaña y día.\")\n",
    "\n",
    "    # max_results es el número de resultados por página (limitado a 100 por la API de Twitter),\n",
    "    # y limit es el número de páginas\n",
    "    for response in tweepy.Paginator(client.search_recent_tweets, query=query, max_results=100, \n",
    "    start_time=targetDay, end_time=currentDay, expansions= 'author_id', user_fields=['location'], \n",
    "    limit=math.ceil(numTweets/100)):\n",
    "\n",
    "        # Sólo quiero analizar un máximo de un tweet por persona\n",
    "        # Si un usuario ha escrito varios tweets, aparece varias veces en la lista de tweets pero sólo una\n",
    "        # en la lista de usuarios, por lo que necesito un contador separado para esta lista\n",
    "        contadorUsuarios = 0\n",
    "\n",
    "        for i in range(response.meta['result_count']):\n",
    "\n",
    "            autor = response.data[i]['author_id']\n",
    "            if autor in listaUsuarios:            \n",
    "                continue\n",
    "            listaUsuarios.append(autor)\n",
    "\n",
    "            tweetIDs.append(response.data[i].id)\n",
    "            tweetTexts.append(response.data[i].text)\n",
    "            tweetLocations.append(response.includes['users'][contadorUsuarios].location)\n",
    "\n",
    "            contadorUsuarios += 1\n",
    "\n",
    "    # Obtenemos también el número total de tweets, por si resultara ser superior al límite de tweets\n",
    "    totalTweets = client.get_recent_tweets_count(query=query, start_time = targetDay, end_time=currentDay, \n",
    "    granularity='day').meta['total_tweet_count']\n",
    "\n",
    "    return (tweetIDs, tweetTexts, tweetLocations, totalTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Realizar análisis\n",
    "# -----------------------------------------------------------------------------\n",
    "def analyzeText(texts):\n",
    "\n",
    "    def getClassification(result):\n",
    "        # La label es un string (p.ej. '5 stars'), hago [0] para seleccionar sólo el número\n",
    "        label = int(result['label'][0])\n",
    "        score = result['score']\n",
    "\n",
    "        # El tweet es \"neutral\" si su puntuación es de 3 estrellas, o bien si el modelo\n",
    "        # no nos da suficiente precisión para considerarlo positivo o negativo\n",
    "        if label == 3 or score < 0.3:\n",
    "            return 0\n",
    "        elif label > 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    return [getClassification(result) for result in classifier(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Obtener las ciudades a partir de la información de localización\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def getCities(tweetLocations):\n",
    "\n",
    "    def normalizeCityName(s):\n",
    "        return s.lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
    "\n",
    "    listaProvincias = [\"A Coruña\", \"Álava\", \"Albacete\", \"Alicante\", \"Almería\", \"Asturias\", \"Ávila\", \"Badajoz\", \"Baleares\", \"Barcelona\", \"Burgos\", \"Cáceres\", \"Cádiz\", \"Cantabria\", \"Castellón\", \"Ciudad Real\", \"Córdoba\", \"Cuenca\", \"Girona\", \"Granada\", \"Guadalajara\", \"Gipuzkoa\", \"Huelva\", \"Huesca\", \"Jaén\", \"La Rioja\", \"Las Palmas\", \"León\", \"Lérida\", \"Lugo\", \"Madrid\", \"Málaga\", \"Murcia\", \"Navarra\", \"Ourense\", \"Palencia\", \"Pontevedra\", \"Salamanca\", \"Segovia\", \"Sevilla\", \"Soria\", \"Tarragona\", \"Santa Cruz de Tenerife\", \"Teruel\", \"Toledo\", \"Valencia\", \"Valladolid\", \"Vizcaya\", \"Zamora\", \"Zaragoza\"]\n",
    "    listaProvinciasN = [normalizeCityName(provincia) for provincia in listaProvincias]\n",
    "\n",
    "    def findLocationInCityList(location):\n",
    "        if location:\n",
    "            location = normalizeCityName(location)\n",
    "            for provincia in listaProvinciasN:\n",
    "                if provincia in location:\n",
    "                    return provincia\n",
    "        return None\n",
    "\n",
    "    return [findLocationInCityList(location) for location in tweetLocations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Generar estadísticas\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def generateStats(tweetIDs, tweetSentiments, tweetCities, totalTweets):\n",
    "    df = pd.DataFrame({\"id\": tweetIDs, \"sentiment\": tweetSentiments, \"city\": tweetCities})\n",
    "\n",
    "    # Para asegurar la trazabilidad del dato, estas DataFrames se almacenan como backup.\n",
    "    # Para ahorrar en espacio, no almacenamos el texto, ya que se puede obtener\n",
    "    # a partir del tweetID en caso de ser necesario.\n",
    "    df.to_pickle(f\"backups/{dateCode}-{userID}.bk\")\n",
    "\n",
    "    results = {}\n",
    "    multiplier = 1\n",
    "\n",
    "    if totalTweets > limit:\n",
    "        analyzedTweets = len(df)\n",
    "        multiplier = totalTweets / analyzedTweets\n",
    "\n",
    "    def convertFormat(pdSeries):\n",
    "        d = dict(pdSeries)\n",
    "        return (int(d[1]*multiplier) if 1 in d else 0, \n",
    "        int(d[0]*multiplier) if 0 in d else 0, \n",
    "        int(d[-1]*multiplier) if -1 in d else 0)\n",
    "\n",
    "    results['total'] = convertFormat(df['sentiment'].value_counts())\n",
    "\n",
    "    df_cities = df.groupby('city')\n",
    "    vc_cities = df_cities['sentiment'].value_counts()\n",
    "\n",
    "    for city in list(vc_cities.index.get_level_values('city')):\n",
    "        results[city] = convertFormat(vc_cities[city])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Subir la información a Firestore\n",
    "# -----------------------------------------------------------------------------\n",
    "def uploadFirestore(stats):\n",
    "    for city, results in stats.items():\n",
    "        # Firestore no permite documentos vacíos, por lo que añado una variable dummy\n",
    "        # al documento correspondiente a cada día\n",
    "        db.collection('data').document(userID).collection(\"days\").document(dateCode).set({'dummy': True})\n",
    "        db.collection('data').document(userID).collection(\"days\").document(dateCode).collection(\"cities\").document(city).set({\n",
    "            'pos': results[0],\n",
    "            'neut': results[1],\n",
    "            'neg': results[2]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cargando lista de campañas.\n",
      "INFO:root:La campaña Mercadona ya ha sido analizada en la fecha 2022-06-04.\n",
      "INFO:root:Procesando campaña activa: atleti.\n",
      "INFO:root:Recolectada una muestra de 1000 tweets, de un total de 4115.\n",
      "INFO:root:1000 tweets analizados.\n",
      "INFO:root:Estadísticas calculadas.\n",
      "INFO:root:Resultados subidos a Firestore.\n",
      "INFO:root:Campaña atleti analizada.\n",
      "INFO:root:La campaña total ya ha sido analizada en la fecha 2022-06-04.\n",
      "INFO:root:Ejecución completada.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Procesar las campañas\n",
    "# -----------------------------------------------------------------------------\n",
    "campaigns = db.collection('data').stream()\n",
    "\n",
    "logging.info('Cargando lista de campañas.')\n",
    "\n",
    "for doc in campaigns:\n",
    "\n",
    "    campaign = doc.to_dict()\n",
    "    # Ejemplo de formato:\n",
    "    # {'tweetLimit': 1000, \n",
    "    # 'isActive': True, \n",
    "    # 'duration': 100, \n",
    "    # 'userID': 'pruebaEspaña', \n",
    "    # 'query': '#españa', \n",
    "    # 'start': DatetimeWithNanoseconds(2022, 6, 4, 21, 12, 57, 524937, tzinfo=datetime.timezone.utc)}\n",
    "\n",
    "    if campaign['isActive']:\n",
    "\n",
    "        query = campaign['query']\n",
    "        limit = campaign['tweetLimit']\n",
    "        userID = campaign['userID']\n",
    "        lastUpdate = campaign['lastUpdate']\n",
    "\n",
    "        # Comprobar que la campaña sigue estando activa\n",
    "        # En caso contrario, marcarla como inactiva y pasar a la siguiente\n",
    "        if campaign['start'] + datetime.timedelta(days = campaign['duration']) < datetime.datetime.now(tz=datetime.timezone.utc):\n",
    "            doc.reference.update({'isActive': False})\n",
    "            logging.info(f'La campaña {userID} ha caducado; se marca como inactiva.')\n",
    "            continue\n",
    "\n",
    "        # Si ya hemos analizado la campaña hoy, pasar a la siguiente\n",
    "        if lastUpdate == dateCode:\n",
    "            logging.info(f'La campaña {userID} ya ha sido analizada en la fecha {dateCode}.')\n",
    "            continue\n",
    "\n",
    "        logging.info(f'Procesando campaña activa: {userID}.')\n",
    "\n",
    "        tweetIDs, tweetTexts, tweetLocations, totalTweets = collectTweets(query, limit)\n",
    "        \n",
    "        if totalTweets == 0:\n",
    "            logging.info(f'No se han encontrado tweets, pasamos a la siguiente campaña.')\n",
    "            continue\n",
    "\n",
    "        logging.info(f'Recolectada una muestra de {min(limit, totalTweets)} tweets, de un total de {totalTweets}.')\n",
    "\n",
    "        tweetSentiments = analyzeText(tweetTexts)  \n",
    "        tweetCities = getCities(tweetLocations)\n",
    "        logging.info(f'{min(limit, totalTweets)} tweets analizados.')\n",
    "\n",
    "        stats = generateStats(tweetIDs, tweetSentiments, tweetCities, totalTweets)\n",
    "        logging.info(f'Estadísticas calculadas.')\n",
    "\n",
    "        uploadFirestore(stats)\n",
    "        doc.reference.update({'lastUpdate': dateCode})\n",
    "        logging.info(f'Resultados subidos a Firestore.')\n",
    "        logging.info(f'Campaña {userID} analizada.')\n",
    "\n",
    "logging.info(f'Ejecución completada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Obtener trending topics\n",
    "# -----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2ad3a26d86eb54ff421ef59107701d39e527f3370099fee424c33d9c3b1c725"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
