{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrés Fernández\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Importar librerías y credenciales\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    import tweepy\n",
    "    from firebase_admin import credentials, firestore, initialize_app\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "except ImportError:\n",
    "    raise ImportError(\"La aplicación requiere las librerías tweepy, firebase_admin y transformers.\")    \n",
    "\n",
    "try:\n",
    "    import keys\n",
    "except ImportError:\n",
    "    raise ImportError(\"No se encuentra el archivo 'keys.py' con las credenciales de la API de Twitter.\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import datetime, os\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Descargar modelo (sólo si no se ha descargado previamente)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_path = './models/transformers/' \n",
    "\n",
    "if not os.path.exists('./models/transformers'):\n",
    "    \n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "    classifier.save_pretrained(model_path)\n",
    "\n",
    "    logging.info(\"Modelo descargado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inicializando servicios...\n",
      "INFO:root:Firestore inicializado.\n",
      "INFO:root:Tweepy inicializado.\n",
      "INFO:root:Modelo NLP inicializado.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Inicializar servicios\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "logging.info(\"Inicializando servicios...\")\n",
    "\n",
    "# Firestore\n",
    "cred = credentials.Certificate('service-acc-key.json')\n",
    "initialize_app(cred)\n",
    "db = firestore.client()\n",
    "logging.info(\"Firestore inicializado.\")\n",
    "\n",
    "# Tweepy\n",
    "client = tweepy.Client(bearer_token=keys.bearer, access_token=keys.access_token, access_token_secret=keys.access_token_secret)\n",
    "logging.info(\"Tweepy inicializado.\")\n",
    "\n",
    "# Clasificador\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "logging.info(\"Modelo NLP inicializado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentTimezone = datetime.datetime.now().astimezone().tzinfo\n",
    "currentTime = datetime.datetime.now(tz=currentTimezone)\n",
    "currentDay = datetime.datetime(currentTime.year, currentTime.month, currentTime.day, tzinfo=currentTimezone)\n",
    "targetDay = currentDay - datetime.timedelta(days=1)\n",
    "\n",
    "# Fechas en formato ISO (YYYY-MM-DD), que utilizo como identificadores en Firestore\n",
    "dateCode = targetDay.date().isoformat()\n",
    "\n",
    "listaProvincias = [\"A Coruña\", \"Álava\", \"Albacete\", \"Alicante\", \"Almería\", \"Asturias\", \"Ávila\", \"Badajoz\", \"Baleares\", \"Barcelona\", \"Burgos\", \"Cáceres\", \"Cádiz\", \"Cantabria\", \"Castellón\", \"Ciudad Real\", \"Córdoba\", \"Cuenca\", \"Girona\", \"Granada\", \"Guadalajara\", \"Gipuzkoa\", \"Huelva\", \"Huesca\", \"Jaén\", \"La Rioja\", \"Las Palmas\", \"León\", \"Lérida\", \"Lugo\", \"Madrid\", \"Málaga\", \"Murcia\", \"Navarra\", \"Ourense\", \"Palencia\", \"Pontevedra\", \"Salamanca\", \"Segovia\", \"Sevilla\", \"Soria\", \"Tarragona\", \"Santa Cruz de Tenerife\", \"Teruel\", \"Toledo\", \"Valencia\", \"Valladolid\", \"Vizcaya\", \"Zamora\", \"Zaragoza\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 6, 5, 0, 0, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'Romance Summer Time'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Recopilar tweets\n",
    "# -----------------------------------------------------------------------------\n",
    "import math\n",
    "\n",
    "def collectTweets(query, numTweets):    \n",
    "\n",
    "    tweetIDs = []\n",
    "    tweetTexts = []\n",
    "    tweetLocations = []\n",
    "\n",
    "    listaUsuarios = []\n",
    "\n",
    "    if numTweets > 1000:\n",
    "        raise Exception(\"Para evitar gastos, la aplicación no admite analizar más de 1000 tweets por campaña y día.\")\n",
    "\n",
    "    # max_results es el número de resultados por página (limitado a 100 por la API de Twitter),\n",
    "    # y limit es el número de páginas\n",
    "    for response in tweepy.Paginator(client.search_recent_tweets, query=query, max_results=100, \n",
    "    start_time=targetDay, end_time=currentDay, expansions= 'author_id', user_fields=['location'], \n",
    "    limit=math.ceil(numTweets/100)):\n",
    "\n",
    "        # Sólo quiero analizar un máximo de un tweet por persona\n",
    "        # Si un usuario ha escrito varios tweets, aparece varias veces en la lista de tweets pero sólo una\n",
    "        # en la lista de usuarios, por lo que necesito un contador separado para esta lista\n",
    "        contadorUsuarios = 0\n",
    "\n",
    "        for i in range(response.meta['result_count']):\n",
    "\n",
    "            autor = response.data[i]['author_id']\n",
    "            if autor in listaUsuarios:            \n",
    "                continue\n",
    "            listaUsuarios.append(autor)\n",
    "\n",
    "            tweetIDs.append(response.data[i].id)\n",
    "            tweetTexts.append(response.data[i].text)\n",
    "            tweetLocations.append(response.includes['users'][contadorUsuarios].location)\n",
    "\n",
    "            contadorUsuarios += 1\n",
    "\n",
    "    # Obtenemos también el número total de tweets, por si resultara ser superior al límite de tweets\n",
    "    totalTweets = client.get_recent_tweets_count(query=query, start_time = targetDay, end_time=currentDay, \n",
    "    granularity='day').meta['total_tweet_count']\n",
    "\n",
    "    return (tweetIDs, tweetTexts, tweetLocations, totalTweets)\n",
    "\n",
    "\n",
    "# El motivo de devolver la información en este formato y no como un diccionario es que \n",
    "# el classifier funciona más rápido si se le pasan todos los tweets a la vez que de uno en uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Realizar análisis\n",
    "# -----------------------------------------------------------------------------\n",
    "def analyzeText(texts):\n",
    "\n",
    "    def getClassification(result):\n",
    "        # la label es un string (p.ej. '5 stars'), hago [0] para seleccionar sólo el número\n",
    "        label = int(result['label'][0])\n",
    "        score = result['score']\n",
    "\n",
    "        # el tweet es \"neutral\" si su puntuación es de 3 estrellas, o bien si el modelo no da elevada precisión\n",
    "        if label == 3 or score < 0.3:\n",
    "            return 0\n",
    "        elif label > 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    # label = '5 stars', p.ej.; el [0] es el número\n",
    "    return [getClassification(result) for result in classifier(texts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCities(tweetLocations):\n",
    "\n",
    "    def normalizeCityName(s):\n",
    "        return s.lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
    "\n",
    "    listaProvincias = [\"A Coruña\", \"Álava\", \"Albacete\", \"Alicante\", \"Almería\", \"Asturias\", \"Ávila\", \"Badajoz\", \"Baleares\", \"Barcelona\", \"Burgos\", \"Cáceres\", \"Cádiz\", \"Cantabria\", \"Castellón\", \"Ciudad Real\", \"Córdoba\", \"Cuenca\", \"Girona\", \"Granada\", \"Guadalajara\", \"Gipuzkoa\", \"Huelva\", \"Huesca\", \"Jaén\", \"La Rioja\", \"Las Palmas\", \"León\", \"Lérida\", \"Lugo\", \"Madrid\", \"Málaga\", \"Murcia\", \"Navarra\", \"Ourense\", \"Palencia\", \"Pontevedra\", \"Salamanca\", \"Segovia\", \"Sevilla\", \"Soria\", \"Tarragona\", \"Santa Cruz de Tenerife\", \"Teruel\", \"Toledo\", \"Valencia\", \"Valladolid\", \"Vizcaya\", \"Zamora\", \"Zaragoza\"]\n",
    "    listaProvinciasN = [normalizeCityName(provincia) for provincia in listaProvincias]\n",
    "\n",
    "    def findLocationInCityList(location):\n",
    "        if location:\n",
    "            location = normalizeCityName(location)\n",
    "            for provincia in listaProvinciasN:\n",
    "                if provincia in location:\n",
    "                    return provincia\n",
    "        return None\n",
    "\n",
    "    return [findLocationInCityList(location) for location in tweetLocations]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Generar estadísticas\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def generateStats(tweetIDs, tweetSentiments, tweetCities, totalTweets):\n",
    "    df = pd.DataFrame({\"id\": tweetIDs, \"sentiment\": tweetSentiments, \"city\": tweetCities})\n",
    "\n",
    "    # Para asegurar la trazabilidad del dato, estas DataFrames se almacenan en la base de datos.\n",
    "    # Se almacenan como un sólo objeto para no tener que hacer varias lecturas y escrituras de BDD por tweet.\n",
    "    # También para ahorrar en espacio, no almacenamos el texto, ya que se puede obtener\n",
    "    # a partir del tweetID en caso de ser necesario.\n",
    "    df.to_pickle(f\"backups/{dateCode}-{userID}.bk\")\n",
    "\n",
    "    results = {}\n",
    "    multiplier = 1\n",
    "\n",
    "    if totalTweets > limit:\n",
    "        analyzedTweets = len(df)\n",
    "        multiplier = totalTweets / analyzedTweets\n",
    "        # multiplicar todos los resultados por el multiplier\n",
    "\n",
    "    def convertFormat(pdSeries):\n",
    "        d = dict(pdSeries)\n",
    "        return (int(d[1]*multiplier) if 1 in d else 0, \n",
    "        int(d[0]*multiplier) if 0 in d else 0, \n",
    "        int(d[-1]*multiplier) if -1 in d else 0)\n",
    "\n",
    "    results['total'] = convertFormat(df['sentiment'].value_counts())\n",
    "\n",
    "    df_cities = df.groupby('city')\n",
    "    vc_cities = df_cities['sentiment'].value_counts()\n",
    "\n",
    "    for city in list(vc_cities.index.get_level_values('city')):\n",
    "        results[city] = convertFormat(vc_cities[city])\n",
    "\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadFirestore(stats):\n",
    "    for city, results in stats.items():\n",
    "        db.collection('data').document(userID).collection(\"days\").document(dateCode).collection(\"cities\").document(city).set({\n",
    "            'pos': results[0],\n",
    "            'neut': results[1],\n",
    "            'neg': results[2]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cargando lista de campañas.\n",
      "INFO:root:La campaña Mercadona ya ha sido analizada en la fecha 2022-06-04.\n",
      "INFO:root:Ejecución completada.\n"
     ]
    }
   ],
   "source": [
    "campaigns = db.collection('data').stream()\n",
    "\n",
    "logging.info('Cargando lista de campañas.')\n",
    "\n",
    "for doc in campaigns:\n",
    "\n",
    "    campaign = doc.to_dict()\n",
    "    # Ejemplo de formato:\n",
    "    # {'tweetLimit': 1000, \n",
    "    # 'isActive': True, \n",
    "    # 'duration': 100, \n",
    "    # 'userID': 'pruebaEspaña', \n",
    "    # 'query': '#españa', \n",
    "    # 'start': DatetimeWithNanoseconds(2022, 6, 4, 21, 12, 57, 524937, tzinfo=datetime.timezone.utc)}\n",
    "\n",
    "    if campaign['isActive']:\n",
    "\n",
    "        query = campaign['query']\n",
    "        limit = campaign['tweetLimit']\n",
    "        userID = campaign['userID']\n",
    "        lastUpdate = campaign['lastUpdate']\n",
    "\n",
    "        # Comprobar que la campaña sigue estando activa\n",
    "        # En caso contrario, marcarla como inactiva y pasar a la siguiente\n",
    "        if campaign['start'] + datetime.timedelta(days = campaign['duration']) < datetime.datetime.now(tz=datetime.timezone.utc):\n",
    "            doc.reference.update({'isActive': False})\n",
    "            logging.info(f'La campaña {userID} ha caducado; se marca como inactiva.')\n",
    "            continue\n",
    "\n",
    "        if lastUpdate == dateCode:\n",
    "            logging.info(f'La campaña {userID} ya ha sido analizada en la fecha {dateCode}.')\n",
    "            continue\n",
    "\n",
    "        logging.info(f'Procesando campaña activa: {userID}.')\n",
    "\n",
    "        tweetIDs, tweetTexts, tweetLocations, totalTweets = collectTweets(query, limit)\n",
    "        \n",
    "        if totalTweets == 0:\n",
    "            logging.info(f'No se han encontrado tweets, pasamos a la siguiente campaña.')\n",
    "            continue\n",
    "\n",
    "        logging.info(f'Recolectada una muestra de {min(limit, totalTweets)} tweets, de un total de {totalTweets}.')\n",
    "\n",
    "        tweetSentiments = analyzeText(tweetTexts)  \n",
    "        tweetCities = getCities(tweetLocations)\n",
    "        logging.info(f'{min(limit, totalTweets)} tweets analizados.')\n",
    "\n",
    "        stats = generateStats(tweetIDs, tweetSentiments, tweetCities, totalTweets)\n",
    "        logging.info(f'Estadísticas calculadas.')\n",
    "\n",
    "        uploadFirestore(stats)\n",
    "        doc.reference.update({'lastUpdate': dateCode})\n",
    "        logging.info(f'Resultados subidos a Firestore.')\n",
    "        logging.info(f'Campaña {userID} analizada.')\n",
    "\n",
    "logging.info(f'Ejecución completada.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2ad3a26d86eb54ff421ef59107701d39e527f3370099fee424c33d9c3b1c725"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
